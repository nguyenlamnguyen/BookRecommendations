{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract data from a book's page on the website\n",
    "def get_book_info(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    # text_stack = soup.find('div', {'id': 'description'})\n",
    "    # text_stack.i.extract()\n",
    "\n",
    "    data = {'title': soup.find('h1', {'class': 'gr-h1 gr-h1--serif'}).text.replace('\\n', '').strip(),\n",
    "            'author': soup.find('a', {'class': 'authorName'}).text.replace('\\n', '').strip(),\n",
    "            # 'ISBN':  soup.find('div', {'id': 'sel-buy-box'}).find('span', {'class': 'buy-box--isbn'}).text[6:],\n",
    "            'summary': soup.find('div', {'id': 'description'}).text.replace('\\n', ''),\n",
    "             'image': soup.find('img', {'id': 'coverImage'})['src']\n",
    "            }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page  64\n",
      "Processing page  65\n",
      "Processing page  66\n",
      "Processing page  67\n",
      "Processing page  68\n",
      "Processing page  69\n",
      "Processing page  70\n",
      "Processing page  71\n",
      "Processing page  72\n",
      "Processing page  73\n",
      "Processing page  74\n",
      "Processing page  75\n",
      "Processing page  76\n",
      "Processing page  77\n",
      "Processing page  78\n",
      "Processing page  79\n",
      "Processing page  80\n",
      "Processing page  81\n",
      "Processing page  82\n",
      "Processing page  83\n",
      "Processing page  84\n",
      "Processing page  85\n",
      "Processing page  86\n",
      "Processing page  87\n",
      "Processing page  88\n",
      "Processing page  89\n",
      "Processing page  90\n"
     ]
    }
   ],
   "source": [
    "# get data about most popular books on the website\n",
    "full_data = []\n",
    "for page in range(64,91):\n",
    "    print('Processing page ',page)\n",
    "    page = requests.get('https://www.goodreads.com/list/show/1.Best_Books_Ever?page={}'.format(page))\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    for book in soup.find_all('a', {'class': 'bookTitle'}):\n",
    "        base_url = 'https://www.goodreads.com'\n",
    "        url = base_url + book['href']\n",
    "        try:\n",
    "            full_data.append(get_book_info(url))\n",
    "        except Exception as e:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1276"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data\n",
    "import pickle\n",
    "\n",
    "with open('raw_books_from_page64.pkl', 'wb') as f:\n",
    "    pickle.dump(full_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
